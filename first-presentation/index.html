<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="simple">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section style="text-align:left">
                    <h4> Robust Adversarial Reinforcement Learning </h4>
                    <h5>Topics in Machine Learning,  <em>Monsoon '17</em></h5>
                    <p>Course Project</p>
					<p>
                        <ul>
                            <li>Binu Jasim T</li>
                            <li>Jerin Philip</li>
                            <li>Punyaslok Pattnaik</li>
                        </ul>
					</p>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <h4> Problem </h4>
                    <script type="text/template">
                    ** Adapt RL agents to model differences. **
                    >  e.g. a Robot  trained to walk on carpet generalize to walk on ice (Test scenario) 

                    ** Transfer learning: Simulation $\rightarrow$ Real World **
                    > It is expensive to train in the real world.

                    </script>
                </section>
                
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Intuitions

                        * Train an agent for the worst case using a destabilizing adversary.
                            * Drawn from friction and mass as something which exerts a disturbing force.
                        * Pit a protagonist and adversary against each other so
                        protagonist learns to generalize on unforeseen
                        situations.


                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Modelling

                        * Modelling errors as extra force in the system.
                        * Protagonist v/s Adversary - **ZERO-SUM GAME**.
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ### Previous Approaches

                        * The targeted problems like Robot walking  are **continuous state space** problems
                        * They can be solved using **Policy Optimization** techniques
                        * Standard Batch Policy algorithms can be used.
                            * REINFORCE
                            * Natural Policy Gradients
                            * TRPO : Trust Region Policy Optimization

                        

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Proposed Algorithm: RARL
                        * Don't exhaustively modelling all possible differences.
                        * Learn **a policy robust to differences** instead.
                        * **Superpowers** to adversary 
                            * Abilities to change enviroment parameters abruptly.
                            * Makes action space of adversary $\neq$ action space of protagonist.

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## What do we expect?
                        Such a protagonist will perform well for robustness evaluations, and in general.
                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Formulation

                        At every timestep $t$
                        * Both players observe state $s_t$.
                        * Takes actions 
                            * $a_t^1 \sim  \mu(s_t) $ 
                            * $a_t^2 \sim \nu(s_t) $.

                        * The state transition is given by environment as:

                        $$ s\_{t+1} = P(s\_t, a\_t^1, a\_t^2) $$
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Formulation

                        Each step of the MDP is given by

                        $$ (s\_t, a\_t^1, a\_t^2, r\_t^1, r\_t^2, s\_{t+1}) $$

                        Protagonist tries to maximize the following reward function:

                        $$ 
                        R^1 = E\_{
                                s\_0 \sim \rho, 
                                a^1 \sim \mu(s),
                                a^2 \sim \nu(s)}
                                {[\sum\_{t=0}^{T-1}{r^1(s, a^1, a^2)}]}
                        $$

                        The adversary on the other hand, tries to minimize the above.
                    </script>
                </section>

                <section style="text-align:left">
                    <h2> Algorithm </h2>
                    <img src="images/algo.png" alt="algorithm" width="50%"> </img>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                    ## Policy Optimization

                    * Directly optimize policy instead of $V(s)$ or $Q(s, a)$.
                    * Policy Optimization: State $\rightarrow$ Action Distribution

                    $$ \max\_{\theta} 
                            {\mathbb{E}[
                                \sum
                                \_{t=0} 
                                ^{H}
                                {R(s\_t) | \pi\_\theta}
                               ]
                            }
                    $$

                    * Policy Gradient:

                    $$ 
                    \begin{align}
                    U(\theta) &= \sum
                                    \_{\tau}
                                    {P(\tau; \theta) R(\tau)} \\\\
                    \nabla\_\theta U(\theta) &= \sum\_{\tau}{P(\tau; \theta) \nabla\_\theta \log P(\tau; \theta) R(\tau)}
                    \end{align}
                    $$

                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation

                        ### Building Blocks

                        * **OpenAI gym** for environments.
                        * **rllab** as deep RL framework aid.



                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation 
                        ### Inputs
                        * An action is a vector containing values of forces.
                            * Forces at multiple joints.
                            * Forces have dimensions: 1D, 2D, 3D.
                            * Concatenation of all the forces is the action.
                            * A distribution predicting the action vector is learnt.

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation
                        ### Inputs

                        A neural network models the required predictive distribution of $\pi(a |s) $.

                        We train based on the rewards the agent obtain, using Policy Optimizer.

                        Thus for a given state $s$ from environment, we get an action, $a$.
                    </script>
                </section>
                <section style="text-align:left">
                    <h4> Example </h4>

                    <p> Inverted Pendulum. </p>
                    <ul>
                        <li> Protagonist: One 1D force as action. </li>
                        <li> Adversary: One 2D force as action. </li>
                    </ul>

                    <img src="images/inverted_pendulum.png" alt="inverted-pendulum"></img>


                </section>
                <section style="text-align:left">
                    <h4> Example </h4>

                    <p> Other examples </p>
                    <img src="images/other_examples.png" alt="other examples"></img>

                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation
                        ### Outputs

                        In an episode, from the environment, we obtain
                        * Velocities, Positions which determine the state.
                        * Reward for the agent - protagonist or adversary is formulated based on state and episode.
                            * Height
                            * Distance covered.
                            * Time staying alive.
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Evaluation

                        ##### average reward vs iterations
                        * Doesn't indicate how robust the policy.
                        * But can be used to benchmark against baselines.

                        ---

                        ##### average reward vs percentile trained policies
                        * Indicates robustness. 
                        * Better rewards over lesser percentiles, more robust learner.

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Evaluation

                        #### Correlate with known parameters.
                        * Mass vs Rewards
                        * Friction vs Rewards
                    </script>
                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
                   	{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }

				],

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },

			});
		</script>
	</body>
</html>
