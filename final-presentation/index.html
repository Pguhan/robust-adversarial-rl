<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="simple">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section style="text-align:left">
                    <h3> Robust Adversarial Reinforcement Learning </h3>
                    <h5>Topics in Machine Learning </h5>
                    <p>Monsoon '17, Course Project</p>
					<p>
                        <ul>
                            <li>Binu Jasim T</li>
                            <li>Jerin Philip</li>
                            <li>Punyaslok Pattnaik</li>
                        </ul>
					</p>
                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        # Outline

                        ### 1. Problem Statement
                        ### 2. Formulation
                        ### 3. Theory: Policy Gradients
                        ### 4. Implementation Specifics
                        ### 5. Results
                        ### 6. Demos and Stats

                    </script>
                </section>
                <section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                    ## Problem
                    **Can a robot trained under normal conditions walk in slippery ice?**
                    * *Adapt RL agents to model differences.*

                    **How well can the model's learning be transferred
                    to real world?**

                    ---

                    ## Intuitions

                    * Train an agent for the worst case in presense of a destabilizing adversary.


                    </script>
                </section>
                
                
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Proposed Algorithm 
                        ### Robust Adversarial Reinforcement Learning
                        * Don't exhaustively model all possible differences.
                        * Learn **a policy robust to differences** instead.

                        * **Superpowers** to adversary 
                            * Abilities to change enviroment parameters abruptly.
                            * Makes action space of adversary $\neq$ action space of protagonist.

                    </script>
                </section>
                </section>

                <section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Formulation

                        At every timestep $t$
                        * Both players observe state $s_t$.
                        * Takes actions 
                            * $a_t^1 \sim  \mu(s_t) $ 
                            * $a_t^2 \sim \nu(s_t) $.

                        * The state transition is given by environment as:

                        $$ s\_{t+1} = P(s\_t, a\_t^1, a\_t^2) $$
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Formulation

                        Each step of the MDP is given by

                        $$ (s\_t, a\_t^1, a\_t^2, r\_t^1, r\_t^2, s\_{t+1}) $$

                        Protagonist tries to maximize the following reward function:

                        $$ 
                        R^1 = E\_{
                                s\_0 \sim \rho, 
                                a^1 \sim \mu(s),
                                a^2 \sim \nu(s)}
                                {[\sum\_{t=0}^{T-1}{r^1(s, a^1, a^2)}]}
                        $$

                        The adversary on the other hand, tries to minimize the above.
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Minimax Formulation

                        \begin{align}
                        R\_1^* &= \min\_{\mu}{\max\_{\nu}{R\_1(\mu, \nu)}} \\\\
                        &= \max\_{\nu}{\min\_{\mu}{R\_1(\mu, \nu)}}
                        \end{align}

                        * Solving the above is computationally intensive.
                        * Searching the tree using $\alpha - \beta$
                        pruning is infeasible.
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Workaround
                        * Focus on learning stationary policies.
                        * Alternate: Monte Carlo Tree Search
                            * Episode rollouts with limited depth.
                            * Estimates for computationally feasible
                            depth.
                            * Sample multiple times and use the estimate
                            mean value - used for policy gradients.


                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                    ## Algorithm 

                    ```py

                    policy = {
                        protagonist: GaussianMLP,
                        adversary: GaussianMLP or Random or Uniform or Constant
                    }

                    optimize = REINFORCE or NaturalPolicyOptimizer or TRPO

                    def train(environment, policy):
                        for i in range(iterations):
                            states, actions, rewards = monte-carlo-rollout(environments, policy)
                            optimize(policy[protagonist], states, actions, rewards)

                            states, actions, rewards = monte-carlo-rollout(environments, policy)
                            optimize(policy[adversary], states, actions, rewards)

                    ```
                    </script>
                </section>
                </section>


                <section>
                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                        ## Policy Optimization

                        * Directly optimize policy -  $\pi\_{\theta}: S \rightarrow A$
                        * Objective

                        $$ \max\_{\theta} 
                                {\mathbb{E}[
                                    \sum
                                    \_{t=0} 
                                    ^{H}
                                    {R(s\_t) | \pi\_\theta}
                                   ]
                                }
                        $$

                        * Methods:
                            * REINFORCE / Vanilla Policy Gradient
                            * Natural Policy Gradient
                            * Trust Region Policy Optimization
                        </script>
                    </section>
                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                        ### Vanilla Policy Gradient

                        $$ 
                        \begin{align}
                        U(\theta) &= \sum
                                        \_{\tau}
                                        {P(\tau; \theta) R(\tau)} \\\\
                        \nabla\_\theta U(\theta) &= \sum\_{\tau}{P(\tau; \theta) \nabla\_\theta \log P(\tau; \theta) R(\tau)}
                        \end{align}
                        $$

                        * Forms basis for **REINFORCE** algorithm.

                        $$
                        \begin{align}
                        \textbf{w}\_{\theta} &=
                        \textbf{w}\_{\theta\_{old}} - \alpha \nabla\_\theta (\dots) 
                        \end{align}
                        $$

                        </script>
                    </section>
                    
                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                        * Derivation
                            $$
                            \begin{align}
                            \nabla\_{\theta}U(\theta) &= \nabla\_{\theta} \sum\_{\tau} P(\tau;\theta)R(\tau) \\\\
                             &= \sum\_{\tau} \frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla\_{\theta}P(\tau;\theta)R(\tau)\\\\
                             &= \sum\_{\tau}P(\tau;\theta)\nabla\_{\theta}\log P(\tau;\theta)R(\tau) \\\\
                             &= \nabla U(\theta) \approx \frac{1}{m} \sum\_{i=1}^{m} \nabla\_{\theta}\log P(\tau^i;\theta)R(\tau^i)
                            \end{align}
                            $$
                        </script>
                    </section>

                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                            ### Advantage
                            * To decrease variance, we can use advantage to a baseline reward. 
                            * We used the one provided by `rllab`.

                            $$ \hat{A} = (R\_t - b\_t)$$

                            * Possible since $\mathbb{E}[b(\tau)] = 0 $
                        </script>
                    </section>

                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                            ### REINFORCE 
                            \begin{align}
                            \nabla\_\theta U(\theta) &= \mathbb{E\_{t}}{[\nabla\_\theta \log \pi(a\_t | s\_t)\hat{A}]} \\\\
                            L(\theta) &= \mathbb{E\_{t}}{[\log \pi(a\_t | s\_t)\hat{A}]}
                            \end{align}

                            * $L$ here is a **surrogate loss**. 
                            * Fed to optimizer to differentiate and propogate gradients.

                        </script>
                    </section>

                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                            ### Importance Sampling Perspective
                            \begin{align}
                            L^{IS}(\theta) &= \mathbb{E\_{t}}{[\frac{\log \pi(a\_t | s\_t)}{\log \pi\_{\text{old}}(a\_t | s\_t)}\hat{A}]}
                            \end{align}

                            \begin{align}
                            \nabla\_{\theta}\log{f(\theta)}\Bigr|\_{\substack{\theta=\theta\_{\text{old}}}} 
                            &= \frac{\nabla\_{\theta}{f(\theta)}\Bigr|\_{\substack{\theta=\theta\_{\text{old}}}}}{f(\theta\_{old})} \\\\
                            &= \nabla\_{\theta}(\frac{{f(\theta)}}{f(\theta\_{old})})\Bigr|\_{\substack{\theta=\theta\_{\text{old}}}}
                            \end{align}
                        </script>
                    </section>
                    <section data-markdown style="text-align:left">
                        <script type="text/template">
                        ### TRPO: Trust Region Policy Optimization

                        \begin{align}
                        \text{maximize} & & \mathbb{E\_{t}}{[\frac{\log \pi(a\_t | s\_t)}{\log \pi\_{\text{old}}(a\_t | s\_t)}\hat{A}]} \\\\
                        \text{subject to}&  & \mathbb{E\_{t}}[D\_{KL}(\pi, \pi\_{\text{old}})] \le \delta 
                        \end{align}

                        * KL-Divergence used here to constrain size of the update.

                        \begin{align}
                        \text{maximize} & & \mathbb{E\_{t}}{
                            [\frac
                                {\log \pi(a\_t | s\_t)}
                                {\log \pi\_{\text{old}}(a\_t | s\_t)}
                                \hat{A}]} 
                            & - \beta( \mathbb{E\_{t}}[D\_{KL}(\pi, \pi\_{\text{old}})] - \delta )
                        \end{align}

                        </script>
                    </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ### Natural Policy Gradient
                        * Approximation of the unconstrained formulation
                        using lagrangian forms basis.
                        \begin{align}
                        \text{maximize} & & & \mathbb{E\_{t}}{ [\frac {\log \pi(a\_t | s\_t)} {\log \pi\_{\text{old}}(a\_t | s\_t)} \hat{A}]}  - \beta( \mathbb{E\_{t}}[D\_{KL}(\pi, \pi\_{\text{old}})] - \delta ) \\\\
                            & & & g \cdot (\theta - \theta\_{\text{old}}) -
                            \frac{\beta}{2} F(\theta -
                            \theta\_{\text{old}})
                        \end{align}

                        Where $g$ is a first order approximation of
                        expectation and $F$ is a second order approximation of
                        $D\_{KL}$.

                        Solution:

                        $$ \theta - \theta\_{\text{old}} =
                        \frac{1}{\beta}\mathbf{F}^{-1}g $$

                        where $\mathbf{F}$ is the **Fisher Information matrix**.


                    </script>
                </section>
                </section>

                <section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation

                        * OpenAI Gym.

                        ```py

                        import gym

                        policy = GaussianMLPPolicy()
                        environment = gym.env('InvertedPendulumAdv-v1')
                        state = gym.make() # Returns start state.

                        for t in episode:
                            action = policy.get_action(state)
                            state = environment.step(action)

                        ```

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation
                        * Modified gym environments to enable superpowered
                        adversaries. 
                        * Modified rllab optimizers (`TRPO`, `REINFORCE`).
                        * `GaussianMLPPolicy` in Lasagne.
                            * Dimensions: [dim(state), 100, 100, 100, dim(action)]

                    </script>
                </section>
                <section style="text-align:left">
                    <h2> Gaussian MLP Policy </h2>
                    <img src="images/network.svg" alt="inverted-pendulum"></img>
                </section>


                <section style="text-align:left">
                    <h4> InvertedPendulum: Parameters Illustrated </h4>

                    <img src="images/process.png" alt="other examples"></img>

                </section>
                <section style="text-align:left">
                    <h4> Additional Agents </h4>

                    <img src="images/other_examples.png" alt="other examples"></img>


                    <!-- <p> No adversarial version of ant - therefore
                        discarded. </p> -->

                </section>
                <section>
                </section>
                    <!--
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation

                        ### Building Blocks

                        * **OpenAI gym** for environments.
                        * **rllab** as deep RL framework aid.



                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation 
                        ### Inputs
                        * An action is a vector containing values of forces.
                            * Forces at multiple joints.
                            * Forces have dimensions: 1D, 2D, 3D.
                            * Concatenation of all the forces is the action.
                            * A distribution predicting the action vector is learnt.

                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation
                        ### Inputs

                        A neural network models the required distribution of $\pi(a |s) $.

                        We train based on the rewards the agent obtain, using Policy Optimizer.

                        Thus for a given state $s$ from environment, we get an action, $a$.
                    </script>
                </section>
                <section style="text-align:left">
                    <h4> Example: Inverted Pendulum </h4>
                    <ul>
                        <li> State Space: 4D position, velocity for both cart and pendulum. </li>
                        <li> Protagonist: One 1D force as action. </li>
                        <li> Adversary: One 2D force on center of pendulum as action. </li>
                    </ul>

                    <img src="images/inverted_pendulum.png" alt="inverted-pendulum"></img>


                </section>
                <section style="text-align:left">
                    <h4> Example </h4>

                    <p> More parameters illustrated: </p>
                    <img src="images/process.png" alt="other examples"></img>

                </section>
                <section style="text-align:left">
                    <h4> Example </h4>

                    <p> Other examples: </p>
                    <img src="images/other_examples.png" alt="other examples"></img>

                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Implementation
                        ### Outputs

                        In an episode, from the environment, we obtain
                        * Velocities, Positions which determine the state.
                        * Reward for the agent - protagonist or adversary is formulated based on state and episode.
                            * Height
                            * Distance covered.
                            * Time staying alive.
                    </script>
                </section>
                -->
                </section>
                <section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Evaluation

                        ##### average reward vs iterations
                        * Doesn't indicate how robust the policy.
                        * But can be used to benchmark against baselines.

                        ---


                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/BASELINE-env-HopperAdv-v1_no_adv_Exp3_Itr500_BS4000_Adv1.0_stp0.01_lam0.97.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/BASELINE-env-InvertedPendulumAdv-v1_no_adv_Exp3_Itr500_BS4000_Adv1.0_stp0.01_lam0.97.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/BASELINE-env-SwimmerAdv-v1_no_adv_Exp3_Itr100_BS4000_Adv1.0_stp0.01_lam0.97.p.png"></img>
                    </script>
                </section>
                <!--
                    :r !find images/results/ -iname '*.png"
                -->
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/BASELINE-env-HopperAdv-v1_no_adv_Exp3_Itr500_BS4000_Adv1.0_stp0.01_lam0.97.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/BASELINE-env-Walker2dAdv-v1_no_adv_Exp3_Itr500_BS4000_Adv1.0_stp0.01_lam0.97.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        <img
                        src="images/results/env-HopperAdv-v1_Exp1_Itr500_BS4000_Adv0.25_stp0.01_lam0.97_364289.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                    <img
                    src="images/results/env-SwimmerAdv-v1_Exp3_Itr50_BS4000_Adv0.25_stp0.01_lam0.97_240911.p.png"></img>
                    </script>
                </section>
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                    <img
                    src="images/results/env-Walker2dAdv-v1_Exp1_Itr500_BS4000_Adv0.25_stp0.01_lam0.97_163843.p.png"></img>
                    </script>
                </section>
                
                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Evaluation
                        ##### average reward vs percentile trained policies
                        * Indicates robustness. 
                        * Better rewards over lesser percentiles, more robust learner.

                    </script>
                </section>

                <section data-markdown style="text-align:left">
                    <script type="text/template">
                        ## Evaluation

                        #### Correlate with known parameters.
                        * Mass vs Rewards
                        * Friction vs Rewards
                    </script>
                </section>

                <!--
                <section style="text-align:left">
                    <div>
                    <img width="49%" src="images/results/hopper_robustness.png"></img>
                    <img width="49%"src="images/results/hopper_robustness.png"></img>
                    </div>
                </section>
                -->
                <section style="text-align:left">
                    <div>
                    <img width="49%" src="images/results/Walker2d - Robust.png"></img>
                    <img width="49%"src="images/results/Waler Robust.png"></img>
                    </div>
                </section>


                </section>
                <section>
                    <section style="text-align:left">
                        <h3> Videos </h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/2pfe-Tod0Co?rel=0" frameborder="0" allowfullscreen></iframe>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/SJr1voQJDyQ?rel=0" frameborder="0" allowfullscreen></iframe>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/0fwI2DSx238?rel=0" frameborder="0" allowfullscreen></iframe>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/2_oBa9yzAgg?rel=0" frameborder="0" allowfullscreen></iframe>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/qYVPmaHu9no?rel=0" frameborder="0" allowfullscreen></iframe>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/DzNvJ3QjE8Q?rel=0" frameborder="0" allowfullscreen></iframe>
                    </section>
                    <section data-markdown style="text-align:left">
                        ### Stats


                        | Model             | Runtime  | Episodes | Iterations per Episode |
                        | ---               | ---      | ---      | ----                   |
                        | Swimmer           | 2 hours  | 3        | 100                    |
                        | Inverted Pendulum | 2 hours  | 3        | 500                    |
                        | Hopper            | 8 hours  | 3        | 500                    |
                        | Walker2d          | 8 hours  | 3        | 500                    |
                        | HalfCheetah       | 14 hours | 3        | 600                    |

                    </section>
                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
                   	{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }

				],

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },

                // The "normal" size of the presentation, aspect ratio will be preserved
                // when the presentation is scaled to fit different resolutions. Can be
                // specified using percentage units.
                width: 1920,
                height: 1080,

                // Factor of the display size that should remain empty around the content
                margin: 0.1,

                // Bounds for smallest/largest possible scale to apply to content
                minScale: 0.1,
                maxScale: 1.5

			});
		</script>
	</body>
</html>
